import os
import time
from datetime import datetime
from tqdm.notebook import tqdm

def generate_responses_for_golden_dataset(golden_master, graph_agent, pause_secs_between_each_run: int = 0):
    for test_row in tqdm(golden_master):
        inputs = {"messages" : [HumanMessage(content=test_row.eval_sample.user_input)]}
        response = graph_agent.invoke(inputs)
    
        evaluation_contexts = extract_contexts_for_eval(response["messages"])
        eval_sample = {
            "user_input": inputs["messages"][0].content,
            "response": response["messages"][-1].content,  # Final AI response
            "retrieved_contexts": evaluation_contexts,
            "tools_used": parsed_data['summary']['tools'],
            "num_contexts": len(evaluation_contexts)
        }
        test_row.eval_sample.response = eval_sample["response"]
        test_row.eval_sample.retrieved_contexts = eval_sample["retrieved_contexts"]
        if pause_secs_between_each_run > 0:
            time.sleep(pause_secs_between_each_run)
    return golden_master


def run_ragas_evaluation(golden_dataset, method_name: str =  "Unknown", evaluator_model: str = "gpt-4.1-mini", 
                         eval_timeout: int = 360, request_timeout: int = 120, custom_metrics: list = None):
    # Convert to RAGAS evaluation dataset
    evaluation_dataset = EvaluationDataset.from_pandas(golden_dataset.to_pandas())

    # Setup evaluator LLM
    evaluator_llm = LangchainLLMWrapper(
                        ChatOpenAI(
                          model=evaluator_model,
                          temperature=0,
                          request_timeout=request_timeout
                        )
                    )
    # Configure evaluation settings
    run_config = RunConfig(timeout=eval_timeout)

    # Use custom metrics if provided, otherwise use default set
    if custom_metrics is None:
        metrics = [
          LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()
        ]
    else:
        metrics = custom_metrics

    print(f"ðŸ§ª Running RAGAS evaluation for {method_name}")

    # Run evaluation
    result = evaluate(
      dataset=evaluation_dataset,
      metrics=metrics,
      llm=evaluator_llm,
      token_usage_parser=get_token_usage_for_openai,
      run_config=run_config
    )

    print(f"âœ… Completed  RAGAS evaluation for {method_name}")
    return result

def record_metrics_from_run(retriever_name, dataframe):
    new_dataframe = dataframe.copy()
    columns=['context_recall', 'faithfulness', 'factual_correctness', 'answer_relevancy', 'context_entity_recall', 'noise_sensitivity_relevant']
    metrics_filename = 'ragas-evaluation-metrics.csv'
    dataset_df = pd.DataFrame()
    if os.path.exists(metrics_filename):
        dataset_df = pd.read_csv(metrics_filename)
    new_dataframe['datetime'] = datetime.now().strftime('%Y-%m-%d %T')
    new_dataframe['retriever'] = retriever_name
    new_dataframe = new_dataframe[['datetime', 'retriever'] + columns]
    dataset_df = pd.concat([dataset_df, new_dataframe])

    dataset_df.to_csv(metrics_filename, index=False)