import os
import logging
from getpass import getpass
from dotenv import load_dotenv
from langchain_community.document_loaders import CSVLoader
from tqdm.notebook import tqdm
import gc
from datetime import datetime

from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader

from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams

from langgraph.graph import START, END, StateGraph
from typing_extensions import List, TypedDict
from langchain_core.documents import Document


# Set up logging with third-party noise suppression
from src.utils.logging_config import setup_logging

logger = setup_logging(__name__)

load_dotenv(dotenv_path=".env")

DATA_FOLDER = os.getenv("DATA_FOLDER")
DEFAULT_FOLDER_LOCATION = "./data/"


def check_if_env_var_is_set(env_var_name: str, human_readable_string: str = "API Key"):
    api_key = os.getenv(env_var_name)

    if api_key:
        logger.info(f"ğŸ”‘ {env_var_name} is present")
    else:
        logger.warning(f"âš ï¸ {env_var_name} is NOT present, prompting for key")
        os.environ[env_var_name] = getpass.getpass(
            f"Please enter your {human_readable_string}: "
        )


check_if_env_var_is_set("OPENAI_API_KEY", "OpenAI API key")
check_if_env_var_is_set("COHERE_API_KEY", "Cohere API key")


#
# ### Data Preparation
#
def load_and_prepare_pdf_loan_docs(folder: str = DEFAULT_FOLDER_LOCATION):
    """
    Load federal student loan PDF documents for hybrid RAG dataset.
    
    **ğŸ¯ PURPOSE & STRATEGY:**
    - Loads authoritative federal student loan policy documents
    - Provides official regulatory content for RAG knowledge base
    - Complements customer complaint data with policy guidance
    - Essential source of ground truth for federal loan procedures
    
    **âš¡ PERFORMANCE CHARACTERISTICS:**
    - **Loading Time**: 10-30 seconds for 4 PDFs (varies by file size)
    - **Memory Usage**: ~15-25MB for document objects
    - **File Format**: PDF documents processed with PyMuPDFLoader
    - **Garbage Collection**: Automatic cleanup to optimize memory
    
    **ğŸ”§ TECHNICAL IMPLEMENTATION:**
    - **Loader**: PyMuPDFLoader for robust PDF text extraction
    - **Directory Scanning**: Loads all *.pdf files from specified folder
    - **Path Resolution**: Flexible path handling for different execution contexts
    - **Memory Management**: gc.collect() after loading for optimization
    
    **ğŸ“Š EXPECTED PDF DOCUMENTS:**
    1. **Academic Calendars & Cost of Attendance** - Timing and cost guidelines
    2. **Applications & Verification Guide** - FAFSA application procedures
    3. **Federal Pell Grant Program** - Grant eligibility and amounts
    4. **Direct Loan Program** - Federal loan types and requirements
    
    **ğŸ—‚ï¸ DOCUMENT STRUCTURE:**
    Each document contains:
    - **page_content**: Extracted text content from PDF pages
    - **metadata**: File path, page numbers, document source information
    - **LangChain Format**: Ready for text splitting and embedding
    
    **ğŸ’¡ HYBRID DATASET ROLE:**
    PDFs provide authoritative "what the policy says" content that pairs with
    CSV complaints showing "what actually happens" for comprehensive coverage.
    
    Args:
        folder (str): Directory path containing PDF files (default: ../data/)
                     Uses DATA_FOLDER env var if set, with fallback path resolution
    
    Returns:
        list: List of LangChain Document objects with extracted PDF content
              Each document represents one PDF page with content and metadata
    
    **ğŸ” PATH RESOLUTION LOGIC:**
    1. Use DATA_FOLDER environment variable if set
    2. Check if provided folder exists
    3. Fallback to "../" + DEFAULT_FOLDER_LOCATION
    4. Log final resolved path for transparency
    
    **âš ï¸ IMPORTANT NOTES:**
    - Requires PDF files in specified directory (fails gracefully if empty)
    - PyMuPDFLoader handles complex PDF structures and formatting
    - Memory usage scales with PDF file sizes and number of pages
    - Documents ready for RecursiveCharacterTextSplitter processing
    
    **ğŸ› ï¸ TROUBLESHOOTING:**
    - **No PDFs found**: Check folder path and file extensions
    - **Loading errors**: Verify PDF file integrity and permissions
    - **Memory issues**: Consider processing PDFs in batches for large collections
    
    Example:
        >>> pdf_docs = load_and_prepare_pdf_loan_docs("./data")
        >>> print(f"Loaded {len(pdf_docs)} PDF pages")
        >>> print(f"First doc content preview: {pdf_docs[0].page_content[:200]}...")
    """
    logger.info(f"ğŸ“ Current working directory: {os.getcwd()}")
    if DATA_FOLDER:
        folder = DATA_FOLDER
    if not os.path.exists(folder):
        folder = DEFAULT_FOLDER_LOCATION
    logger.info(f"ğŸ“„ Loading student loan PDFs from: {folder}")
    loader = DirectoryLoader(folder, glob="*.pdf", loader_cls=PyMuPDFLoader)
    docs = loader.load()
    gc.collect()
    logger.info(f"âœ… Loaded {len(docs)} PDF documents")
    return docs


def load_and_prepare_csv_loan_docs(folder: str = DEFAULT_FOLDER_LOCATION):
    """
    Load and filter customer complaint data for hybrid RAG dataset.
    
    **ğŸ¯ PURPOSE & STRATEGY:**
    - Loads real customer experiences with federal student loan servicers
    - Provides "what actually happens" context to complement policy documents
    - Implements comprehensive data quality pipeline (4,547 â†’ 825 â†’ 480)
    - Essential source of practical, real-world scenarios for RAG responses
    
    **âš¡ PERFORMANCE CHARACTERISTICS:**
    - **Processing Time**: 30-60 seconds for full quality pipeline
    - **Data Reduction**: ~89% filtered out (4,547 â†’ 480 final records)
    - **Memory Usage**: ~10-15MB for final filtered dataset
    - **Quality Focus**: Prioritizes meaningful narratives over quantity
    
    **ğŸ”§ TECHNICAL IMPLEMENTATION:**
    - **CSV Loader**: LangChain CSVLoader with comprehensive metadata extraction
    - **3-Step Pipeline**: Load â†’ Content Setup â†’ Quality Filtering
    - **Quality Filters**: Length, redaction, emptiness validation
    - **Structured Output**: Formatted complaint details for RAG consumption
    
    **ğŸ“Š DATA QUALITY PIPELINE:**
    
    **STEP 1: Raw CSV Loading (4,547 records)**
    - Loads all consumer complaint records from complaints.csv
    - Extracts 18 metadata columns for comprehensive context
    - Initial count typically 4,547 records from source dataset
    
    **STEP 2: Content Preparation (no reduction)**
    - Sets page_content to "Consumer complaint narrative" field
    - Preserves all metadata for contextual information
    - No filtering at this stage - pure content setup
    
    **STEP 3: Quality Filtering (4,547 â†’ 480)**
    - **Length Filter**: Removes narratives < 100 characters
    - **Redaction Filter**: Removes heavily redacted content (>5 XXXX)
    - **Empty Filter**: Removes empty, "None", "N/A" narratives
    - **Multi-Issue Tracking**: Statistics on records with multiple problems
    
    **ğŸ—‚ï¸ OUTPUT DOCUMENT STRUCTURE:**
    Each valid document formatted as:
    ```
    Customer Issue: [Issue from metadata]
    Product: [Product from metadata]
    Complaint Details: [Original narrative text]
    ```
    
    **ğŸ“ˆ TYPICAL FILTER STATISTICS:**
    - **Too short**: ~2,800 records (< 100 chars)
    - **Too many XXXX**: ~500 records (> 5 redactions)
    - **Empty/N/A**: ~1,200 records (no content)
    - **Valid retained**: ~480 records (10.7%)
    
    **ğŸ’¡ HYBRID DATASET ROLE:**
    Complaints provide practical "customer experience" content that pairs with
    PDF policies to show both regulatory requirements and real-world application.
    
    Args:
        folder (str): Directory path containing complaints.csv (default: ../data/)
                     Uses DATA_FOLDER env var if set, with fallback path resolution
    
    Returns:
        list: List of quality-filtered LangChain Document objects
              Each document represents one meaningful customer complaint
              with structured formatting and comprehensive metadata
    
    **ğŸ” METADATA COLUMNS EXTRACTED:**
    - **Core Info**: Date received, Product, Sub-product, Issue, Sub-issue
    - **Content**: Consumer complaint narrative (becomes page_content)
    - **Company Data**: Company, Company public response, Company response
    - **Process Info**: Submitted via, Date sent, Timely response, Consumer disputed
    - **Location**: State, ZIP code
    - **Identifiers**: Complaint ID, Tags, Consumer consent provided
    
    **âš ï¸ IMPORTANT NOTES:**
    - High filter rate (89%) is intentional for quality over quantity
    - Structured formatting optimizes content for RAG retrieval
    - Metadata preserved for contextual understanding and analysis
    - Memory management with gc.collect() for large dataset processing
    
    **ğŸ› ï¸ TROUBLESHOOTING:**
    - **Low retention rate**: Expected behavior due to data quality focus
    - **File not found**: Check complaints.csv exists in specified folder
    - **Memory issues**: Large initial dataset (~4.5K records) requires adequate RAM
    - **Empty results**: Verify CSV has "Consumer complaint narrative" column
    
    **ğŸ“‹ DATA PIPELINE DOCUMENTATION:**
    This function implements the documented 4,547â†’825â†’480 pipeline where:
    - 4,547: Raw CSV records loaded
    - 825: Records with non-empty narratives (CSVLoader automatic filtering)
    - 480: Final quality-filtered records suitable for RAG
    
    Example:
        >>> complaint_docs = load_and_prepare_csv_loan_docs("./data")
        >>> print(f"Loaded {len(complaint_docs)} quality complaint records")
        >>> print(f"Sample complaint: {complaint_docs[0].page_content[:200]}...")
        >>> print(f"Metadata keys: {list(complaint_docs[0].metadata.keys())}")
    """
    logger.info(f"ğŸ“ Current working directory: {os.getcwd()}")
    if DATA_FOLDER:
        folder = DATA_FOLDER
    if not os.path.exists(folder):
        folder = DEFAULT_FOLDER_LOCATION

    loader = CSVLoader(
        file_path=f"{folder}/complaints.csv",
        metadata_columns=[
            "Date received",
            "Product",
            "Sub-product",
            "Issue",
            "Sub-issue",
            "Consumer complaint narrative",
            "Company public response",
            "Company",
            "State",
            "ZIP code",
            "Tags",
            "Consumer consent provided?",
            "Submitted via",
            "Date sent to company",
            "Company response to consumer",
            "Timely response?",
            "Consumer disputed?",
            "Complaint ID",
        ],
    )

    logger.info(f"ğŸ“Š Loading student loan complaints from: {folder}/complaints.csv")

    # STEP 1: Load raw data
    loan_complaint_data = loader.load()
    initial_count = len(loan_complaint_data)
    logger.info(f"ğŸ“‹ STEP 1 - Raw CSV loaded: {initial_count:,} records")

    # STEP 2: Set page content from narrative
    for doc in loan_complaint_data:
        doc.page_content = doc.metadata["Consumer complaint narrative"]

    logger.info(
        f"ğŸ“ STEP 2 - Page content set: {len(loan_complaint_data):,} records (no change)"
    )
    gc.collect()

    # STEP 3: Apply quality filters with detailed tracking
    logger.info(f"ğŸ” STEP 3 - Applying quality filters...")

    filter_stats = {
        "too_short": 0,
        "too_many_xxxx": 0,
        "empty_or_na": 0,
        "multiple_issues": 0,
        "valid": 0,
    }

    filtered_docs = []

    for i, doc in enumerate(loan_complaint_data):
        narrative = doc.metadata.get("Consumer complaint narrative", "")
        issues = []

        # Check each filter condition
        if len(narrative.strip()) < 100:
            filter_stats["too_short"] += 1
            issues.append("length")

        if narrative.count("XXXX") > 5:
            filter_stats["too_many_xxxx"] += 1
            issues.append("redaction")

        if narrative.strip() in ["", "None", "N/A"]:
            filter_stats["empty_or_na"] += 1
            issues.append("empty")

        # Track records with multiple issues
        if len(issues) > 1:
            filter_stats["multiple_issues"] += 1

        # Keep valid records
        if not issues:
            filter_stats["valid"] += 1
            doc.page_content = (
                f"Customer Issue: {doc.metadata.get('Issue', 'Unknown')}\n"
            )
            doc.page_content += f"Product: {doc.metadata.get('Product', 'Unknown')}\n"
            doc.page_content += f"Complaint Details: {narrative}"
            filtered_docs.append(doc)

    # Log detailed filter results
    logger.info(f"ğŸ“Š FILTER RESULTS:")
    logger.info(f"   âŒ Too short (< 100 chars): {filter_stats['too_short']:,}")
    logger.info(f"   âŒ Too many XXXX (> 5): {filter_stats['too_many_xxxx']:,}")
    logger.info(f"   âŒ Empty/None/N/A: {filter_stats['empty_or_na']:,}")
    logger.info(f"   âš ï¸  Multiple issues: {filter_stats['multiple_issues']:,}")

    total_filtered = initial_count - len(filtered_docs)
    retention_rate = (len(filtered_docs) / initial_count) * 100

    logger.info(f"ğŸ“ˆ SUMMARY:")
    logger.info(f"   âœ… Valid records kept: {len(filtered_docs):,}")
    logger.info(f"   ğŸ—‘ï¸  Total filtered out: {total_filtered:,}")
    logger.info(f"   ğŸ“Š Retention rate: {retention_rate:.1f}%")

    gc.collect()
    return filtered_docs.copy()


def split_documents(documents):
    """
    Split hybrid dataset documents into optimal chunks for vector embedding.
    
    **ğŸ¯ PURPOSE & STRATEGY:**
    - Converts variable-length documents into uniform chunks for embedding
    - Optimizes chunk size for both PDF policies and customer complaints
    - Preserves context with overlapping boundaries for semantic continuity
    - Essential preprocessing step for vector database ingestion
    
    **âš¡ PERFORMANCE CHARACTERISTICS:**
    - **Processing Time**: 5-15 seconds for ~1,100 documents
    - **Chunk Creation**: Typically 3-5x document count (varies by content length)
    - **Memory Usage**: Modest increase due to chunk duplication in overlaps
    - **Deterministic**: Same documents always produce same chunks
    
    **ğŸ”§ TECHNICAL IMPLEMENTATION:**
    - **Splitter**: RecursiveCharacterTextSplitter for intelligent boundary detection
    - **Chunk Size**: 750 characters optimized for both PDF and complaint content
    - **Overlap**: 100 characters to preserve context across chunk boundaries
    - **Boundary Detection**: Respects sentence/paragraph boundaries when possible
    
    **ğŸ“Š CHUNKING STRATEGY:**
    - **Size Optimization**: 750 chars balances context vs precision
      - Too small: Loses semantic context
      - Too large: Reduces retrieval precision
    - **Overlap Benefits**: 100 char overlap prevents information loss at boundaries
    - **Content Agnostic**: Works well for both PDF policies and complaint narratives
    
    **ğŸ—‚ï¸ CHUNK CHARACTERISTICS:**
    - **PDF Chunks**: Policy sections, procedures, regulatory text
    - **Complaint Chunks**: Customer scenarios, issues, resolutions
    - **Metadata Preservation**: Original document metadata carried forward
    - **Content Format**: Same structured format as original documents
    
    **ğŸ’¡ EMBEDDING OPTIMIZATION:**
    750-character chunks are optimal for:
    - **OpenAI Embeddings**: Good semantic density without token waste
    - **Retrieval Precision**: Specific enough for accurate matching
    - **Context Completeness**: Large enough for meaningful content
    - **Processing Speed**: Fast embedding generation and search
    
    Args:
        documents (list): List of LangChain Document objects from PDF/CSV loading
                         Typically ~1,100 documents from hybrid dataset
    
    Returns:
        list: List of chunked Document objects ready for vector embedding
              Typically ~3,000-5,000 chunks depending on source content length
    
    **ğŸ” CHUNKING BEHAVIOR:**
    - **Recursive Splitting**: Tries paragraphs, then sentences, then words
    - **Intelligent Boundaries**: Avoids breaking words or sentences when possible
    - **Metadata Inheritance**: Each chunk retains original document metadata
    - **Content Continuity**: Overlaps ensure no information lost at boundaries
    
    **âš ï¸ IMPORTANT NOTES:**
    - Chunk count varies with source document lengths
    - Overlapping content means some text appears in multiple chunks
    - Chunking is deterministic - same input always produces same output
    - Original document structure preserved in metadata
    
    **ğŸ› ï¸ PERFORMANCE CONSIDERATIONS:**
    - Memory usage increases ~20-30% due to overlapping content
    - Processing time scales linearly with document count
    - Chunk size affects downstream embedding and search performance
    - Optimal size determined through testing with hybrid dataset
    
    Example:
        >>> pdf_docs = load_and_prepare_pdf_loan_docs()
        >>> csv_docs = load_and_prepare_csv_loan_docs()
        >>> all_docs = pdf_docs + csv_docs
        >>> chunks = split_documents(all_docs)
        >>> print(f"Split {len(all_docs)} docs into {len(chunks)} chunks")
        >>> print(f"Average chunk size: {sum(len(c.page_content) for c in chunks) / len(chunks):.0f} chars")
    """
    logger.info(
        f"ğŸ“„ Splitting {len(documents)} documents into chunks (size=750, overlap=100)"
    )
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=750, chunk_overlap=100)
    logger.info(
        f"text_splitter: Chunk Size: {text_splitter._chunk_size} | Chunk Overlap: {text_splitter._chunk_overlap}"
    )
    split_docs = text_splitter.split_documents(documents)
    logger.info(f"âœ… Created {len(split_docs)} document chunks")
    return split_docs


def get_vector_store(split_documents):
    """
    Create Qdrant vector database with embedded hybrid dataset chunks.
    
    **ğŸ¯ PURPOSE & STRATEGY:**
    - Creates high-performance vector database for semantic document retrieval
    - Embeds hybrid dataset (PDF policies + customer complaints) for comprehensive search
    - Optimizes for fast similarity search with cosine distance metric
    - Foundation for all RAG retrieval methods in the system
    
    **âš¡ PERFORMANCE CHARACTERISTICS:**
    - **Initialization Time**: 2-5 minutes for ~4,000 chunks (embedding generation)
    - **Memory Usage**: ~39.2MB for complete vector store (embeddings + metadata)
    - **Search Speed**: Sub-second retrieval for k=5 queries
    - **Embedding Model**: OpenAI text-embedding-3-small (1536 dimensions)
    
    **ğŸ”§ TECHNICAL IMPLEMENTATION:**
    - **Database**: Qdrant in-memory deployment for development/testing
    - **Collection**: 'loan_data' collection with cosine similarity
    - **Embeddings**: OpenAI text-embedding-3-small for semantic understanding
    - **Vector Config**: 1536-dimensional vectors with cosine distance
    
    **ğŸ“Š VECTOR STORE SPECIFICATIONS:**
    - **Vector Dimensions**: 1536 (OpenAI text-embedding-3-small)
    - **Distance Metric**: Cosine similarity for semantic search
    - **Storage**: In-memory (:memory:) for fast access
    - **Collection Name**: 'loan_data' for federal student loan content
    
    **ğŸ—‚ï¸ EMBEDDED CONTENT:**
    - **PDF Chunks**: ~615 chunks from federal policy documents
    - **Complaint Chunks**: ~480 chunks from customer complaint narratives
    - **Total Vectors**: ~1,095 embedded document chunks
    - **Content Types**: Policies, procedures, customer scenarios, issues
    
    **ğŸ’¡ DEPLOYMENT STRATEGY:**
    **Current (In-Memory)**: Perfect for development, testing, and MVP
    - **Pros**: Zero setup, fast performance, 39.2MB is optimal size
    - **Cons**: No persistence, single-process only
    
    **Future (Docker/Cloud)**: When scaling needed
    - **When**: Multi-user API, persistence requirements, production deployment
    - **Migration**: Simple environment variable change
    
    **ğŸ” SEARCH CAPABILITIES:**
    - **Semantic Search**: Understands meaning, not just keywords
    - **Hybrid Content**: Searches both policies and customer experiences
    - **Fast Retrieval**: Optimized for k=5 to k=20 document retrieval
    - **Relevance Scoring**: Cosine similarity scores for ranking
    
    Args:
        split_documents (list): List of chunked Document objects from split_documents()
                               Typically ~4,000 chunks from hybrid dataset
    
    Returns:
        QdrantVectorStore: Initialized vector store ready for retrieval
                          Contains embedded chunks with semantic search capabilities
    
    **âš¡ EMBEDDING PROCESS:**
    1. **Client Initialization**: Qdrant in-memory database startup
    2. **Collection Creation**: 'loan_data' collection with vector configuration
    3. **Embedding Generation**: OpenAI API calls to embed all chunks
    4. **Vector Storage**: Chunks stored with embeddings and metadata
    5. **Index Building**: Qdrant optimizes for fast similarity search
    
    **âš ï¸ IMPORTANT NOTES:**
    - Requires OPENAI_API_KEY for embedding generation
    - Embedding cost: ~$0.10-0.20 for complete hybrid dataset
    - In-memory storage lost on process restart (use Docker for persistence)
    - Collection name 'loan_data' used across all retrieval methods
    
    **ğŸ› ï¸ TROUBLESHOOTING:**
    - **Embedding errors**: Check OPENAI_API_KEY and rate limits
    - **Memory issues**: 39MB should work on most systems
    - **Slow performance**: Consider smaller chunk sizes or fewer documents
    - **Collection exists**: Error if collection already created (restart process)
    
    **ğŸ’° COST ANALYSIS:**
    - **One-time embedding**: ~$0.10-0.20 for hybrid dataset
    - **Search queries**: No additional embedding costs
    - **Scaling**: Costs scale linearly with document count
    
    Example:
        >>> chunks = split_documents(combined_docs)
        >>> vector_store = get_vector_store(chunks)
        >>> retriever = vector_store.as_retriever(search_kwargs={"k": 5})
        >>> results = retriever.get_relevant_documents("What is FAFSA?")
        >>> print(f"Found {len(results)} relevant documents")
    """
    logger.info(f"ğŸ—ƒï¸ Starting Qdrant in-memory database")
    client = QdrantClient(":memory:")

    logger.info(f"ğŸ“¦ Creating Qdrant collection 'loan_data'")
    client.create_collection(
        collection_name="loan_data",
        vectors_config=VectorParams(size=1536, distance=Distance.COSINE),
    )

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vector_store = QdrantVectorStore(
        client=client,
        collection_name="loan_data",
        embedding=embeddings,
    )

    logger.info(f"â¬†ï¸ Adding {len(split_documents)} documents to Qdrant collection")
    # We can now add our documents to our vector store.
    vector_store.add_documents(documents=split_documents)
    logger.info(f"âœ… Qdrant vector store ready with {len(split_documents)} documents")
    return vector_store


# Let's define our retriever.


def get_retriever(vector_store):
    return vector_store.as_retriever(search_kwargs={"k": 5})


# Now we can produce a node for retrieval!


def retrieve(state, retriever):
    logger.info(f"ğŸ” Retrieving documents for question: {state['question'][:100]}...")
    retrieved_docs = retriever.invoke(state["question"])
    logger.info(f"ğŸ“š Retrieved {len(retrieved_docs)} relevant documents")
    return {"context": retrieved_docs}


# ### Augmented
#
# Let's create a simple RAG prompt!


def get_rag_prompt():
    RAG_PROMPT = """\
  You are a helpful assistant who answers questions based on provided context. You must only use the provided context, and cannot use your own knowledge.

  ### Question
  {question}

  ### Context
  {context}
  
  Return a confidence score and a reason for the score once as json finished based on the outcome of the query.
  """

    return ChatPromptTemplate.from_template(RAG_PROMPT)


# ### Generation
#
# We'll also need an LLM to generate responses - we'll use `gpt-4o-nano` to avoid using the same model as our judge model.

llm = ChatOpenAI(model="gpt-4.1-nano")


# Then we can create a `generate` node!


def generate(state):
    logger.info(
        f"ğŸ¤– Generating response using {len(state['context'])} context documents"
    )
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = rag_prompt.format_messages(
        question=state["question"], context=docs_content
    )
    response = llm.invoke(messages)
    logger.info(f"âœ… Generated response with {len(response.content)} characters")
    return {"response": response.content}


# ### Building RAG Graph with LangGraph
#
# Let's create some state for our LangGraph RAG graph!


class State(TypedDict):
    question: str
    context: List[Document]
    response: str


graph_builder = StateGraph(State)
graph_builder.add_node("retrieve", retrieve)
graph_builder.add_node("generate", generate)
graph_builder.add_edge(START, "retrieve")
graph_builder.add_edge("retrieve", "generate")
graph_builder.add_edge("generate", END)
graph = graph_builder.compile()
