import logging
from langchain_core.tools import tool
from src.core.graph_rag_functions import (
    naive_graph,
    contextual_compression_graph,
    multi_query_graph,
    parent_document_graph,
)
from langchain_core.messages import HumanMessage

# Set up logging with third-party noise suppression
from src.utils.logging_config import setup_logging

logger = setup_logging(__name__)


@tool
def ask_naive_llm_tool(question: str):
    """ğŸ¯ PRIMARY FEDERAL STUDENT LOAN ASSISTANT - ALWAYS USE FIRST!

    ğŸ›ï¸ COMPREHENSIVE KNOWLEDGE BASE containing:
    âœ… Complete federal student loan policies AND real customer complaint cases
    âœ… Official Department of Education guidelines with practical examples
    âœ… 4,000+ real borrower scenarios covering common issues

    ğŸ¯ ESSENTIAL for ALL student loan questions:
    â€¢ Payment problems and solutions (with real cases)
    â€¢ Forgiveness programs and eligibility criteria
    â€¢ Income-driven repayment plans and calculations
    â€¢ Servicer issues (Nelnet, Aidvantage, Mohela, etc.)
    â€¢ Default prevention and rehabilitation
    â€¢ Application processes and requirements
    â€¢ Policy explanations with customer examples

    âš¡ AUTHORITATIVE SOURCE: Federal policies + real customer experiences
    ğŸª Contains both "what the law says" AND "what actually happens"

    ğŸ”§ TECHNICAL IMPLEMENTATION:
    â€¢ Retrieval: Cosine similarity search (k=5 most relevant chunks)
    â€¢ Chunking: 750 chars with 100 char overlap for optimal context
    â€¢ Embedding: OpenAI text-embedding-3-small (1536 dimensions)
    â€¢ Dataset: ~1,095 chunks (PDF: 615 + Complaints: 480)
    â€¢ Performance: Best RAGAS scores (context_recall: 0.637, faithfulness: 0.905)
    â€¢ Response time: ~1-3 seconds for typical queries

    ğŸ“Š RAGAS EVALUATION RESULTS (All metrics 0-1, higher=better):
    â€¢ Context Recall: 0.637 (retrieval quality)
    â€¢ Faithfulness: 0.905 (factual accuracy)
    â€¢ Answer Relevancy: 0.62 (response relevance)
    â€¢ Factual Correctness: 0.41 (semantic accuracy)

    ğŸ”„ RETURNS:
    dict: {
        "messages": [HumanMessage with generated response],
        "context": [List of 5 Document objects with metadata and relevance scores]
    }

    Use this BEFORE searching external sources - it has the most complete information!
    """
    logger.info(f"ğŸ” [Naive Tool] Processing question: {question[:100]}...")
    response = naive_graph.invoke({"question": question})
    logger.info(
        f"âœ… [Naive Tool] Generated response with {len(response['context'])} contexts"
    )
    return {
        "messages": [HumanMessage(content=response["response"])],
        "context": response["context"],
    }


@tool
def ask_contextual_compression_llm_tool(question: str):
    """ğŸ¯ PREMIUM FEDERAL STUDENT LOAN ASSISTANT - HIGHEST QUALITY ANSWERS!

    ğŸ›ï¸ EXPERT-LEVEL KNOWLEDGE BASE featuring:
    âœ… Advanced reranked federal policies + real customer complaint database
    âœ… Most relevant contexts through AI-powered content ranking
    âœ… Precision-filtered information for complex inquiries

    ğŸ¯ SPECIALIZED for challenging student loan questions:
    â€¢ Complex repayment scenarios requiring expert analysis
    â€¢ Multi-servicer coordination problems
    â€¢ Advanced forgiveness program eligibility
    â€¢ Intricate policy interpretations with real-world examples
    â€¢ Detailed customer complaint resolution strategies

    âš¡ PREMIUM QUALITY: Uses advanced reranking for highest accuracy
    ğŸª Perfect for difficult cases requiring nuanced understanding

    ğŸ”§ TECHNICAL IMPLEMENTATION:
    â€¢ Retrieval: Initial cosine search (k=20) â†’ Cohere rerank-v3.5 â†’ top 5
    â€¢ Reranking: AI-powered relevance scoring for precision filtering
    â€¢ Embedding: OpenAI text-embedding-3-small + Cohere reranking
    â€¢ Dataset: Same hybrid dataset (~1,095 chunks) with enhanced relevance
    â€¢ Performance: Balanced RAGAS scores with high precision
    â€¢ Response time: ~2-4 seconds (includes reranking overhead)

    ğŸ“Š RAGAS EVALUATION RESULTS (All metrics 0-1, higher=better):
    â€¢ Context Recall: ~0.60 (good retrieval quality)
    â€¢ Faithfulness: ~0.85 (high factual accuracy)
    â€¢ Answer Relevancy: ~0.65 (enhanced response relevance)
    â€¢ Context Precision: Higher due to reranking (signal-to-noise ratio)

    ğŸ”„ RETURNS:
    dict: {
        "messages": [HumanMessage with generated response],
        "context": [List of 5 reranked Document objects with enhanced relevance scores]
    }

    Preferred for complex inquiries requiring the most accurate information!
    """
    logger.info(
        f"ğŸ” [Contextual Compression Tool] Processing question: {question[:100]}..."
    )
    response = contextual_compression_graph.invoke({"question": question})
    logger.info(
        f"âœ… [Contextual Compression Tool] Generated response with {len(response['context'])} reranked contexts"
    )
    return {
        "messages": [HumanMessage(content=response["response"])],
        "context": response["context"],
    }


@tool
def ask_multi_query_llm_tool(question: str):
    """ğŸ¯ COMPREHENSIVE FEDERAL STUDENT LOAN RESEARCH ASSISTANT!

    ğŸ›ï¸ MULTI-PERSPECTIVE KNOWLEDGE BASE providing:
    âœ… Expanded query analysis across federal policies + customer cases
    âœ… Multiple search angles for thorough coverage
    âœ… Comprehensive answers from diverse information sources

    ğŸ¯ PERFECT for broad student loan research:
    â€¢ Questions requiring multiple policy perspectives
    â€¢ Comprehensive overviews of loan programs
    â€¢ Comparative analysis of repayment options
    â€¢ Thorough investigation of customer issues across servicers
    â€¢ Complete research on forgiveness program alternatives

    âš¡ THOROUGH COVERAGE: Multiple query expansion for complete answers
    ğŸª Best for questions needing comprehensive, multi-faceted responses

    ğŸ”§ TECHNICAL IMPLEMENTATION:
    â€¢ Retrieval: LLM-generated query expansion (3-5 alternative queries)
    â€¢ Search Strategy: Multiple similarity searches combined and deduplicated
    â€¢ Embedding: OpenAI text-embedding-3-small across expanded queries
    â€¢ Dataset: Same hybrid dataset with broader coverage approach
    â€¢ Performance: Good coverage, moderate precision
    â€¢ Response time: ~3-5 seconds (multiple query processing)

    ğŸ“Š RAGAS EVALUATION RESULTS (All metrics 0-1, higher=better):
    â€¢ Context Recall: ~0.55 (good broad coverage)
    â€¢ Faithfulness: ~0.80 (solid factual accuracy)
    â€¢ Answer Relevancy: ~0.58 (comprehensive but sometimes broader)
    â€¢ Context Entity Recall: Higher due to multiple query angles

    ğŸ”„ RETURNS:
    dict: {
        "messages": [HumanMessage with comprehensive response],
        "context": [List of Document objects from multiple query angles]
    }

    Use when you need the most complete possible answer to complex questions!
    """
    logger.info(f"ğŸ” [Multi-Query Tool] Processing question: {question[:100]}...")
    response = multi_query_graph.invoke({"question": question})
    logger.info(
        f"âœ… [Multi-Query Tool] Generated response with {len(response['context'])} contexts from expanded queries"
    )
    return {
        "messages": [HumanMessage(content=response["response"])],
        "context": response["context"],
    }


@tool
def ask_parent_document_llm_tool(question: str):
    """ğŸ¯ EXPERT FEDERAL STUDENT LOAN CONSULTANT - SMALL-TO-BIG RETRIEVAL!

    ğŸ›ï¸ PREMIUM KNOWLEDGE ARCHITECTURE featuring:
    âœ… Advanced small-to-big retrieval with federal policies + customer data
    âœ… Complete document context preservation
    âœ… Enhanced context through parent document access

    ğŸ¯ ELITE expertise for ALL student loan scenarios:
    â€¢ Detailed policy guidance with full regulatory context
    â€¢ Complete customer complaint analysis and resolution
    â€¢ Comprehensive servicer-specific procedures and solutions
    â€¢ Advanced forgiveness program strategies with precedents
    â€¢ Expert-level troubleshooting for complex cases

    âš¡ SMALL-TO-BIG STRATEGY: Find specific chunks, retrieve full context
    ğŸª Combines precision retrieval with comprehensive context

    ğŸ”§ TECHNICAL IMPLEMENTATION:
    â€¢ Retrieval: Small chunk search (512 chars) â†’ retrieve parent docs (750 chars)
    â€¢ Strategy: Precise targeting with expanded context window
    â€¢ Embedding: OpenAI text-embedding-3-small on smaller chunks
    â€¢ Dataset: Same hybrid dataset with hierarchical chunk structure
    â€¢ Performance: Variable results, context-dependent effectiveness
    â€¢ Response time: ~2-4 seconds (two-stage retrieval)

    ğŸ“Š RAGAS EVALUATION RESULTS (All metrics 0-1, higher=better):
    â€¢ Context Recall: 0.268 (lower precision in current evaluation)
    â€¢ Faithfulness: 0.653 (moderate factual accuracy)
    â€¢ Answer Relevancy: Variable (depends on parent document relevance)
    â€¢ Context Preservation: Higher due to full document access

    ğŸ”„ RETURNS:
    dict: {
        "messages": [HumanMessage with context-rich response],
        "context": [List of parent Document objects with full context]
    }

    EXPERIMENTAL: Small-to-big retrieval for comprehensive context!
    """
    logger.info(f"ğŸ” [Parent Document Tool] Processing question: {question[:100]}...")
    response = parent_document_graph.invoke({"question": question})
    logger.info(
        f"âœ… [Parent Document Tool] Generated response with {len(response['context'])} full document contexts"
    )
    return {
        "messages": [HumanMessage(content=response["response"])],
        "context": response["context"],
    }
