{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_fLDElOVoop"
   },
   "source": [
    "## Task 1:  Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_env_var_is_set(env_var_name: str, human_readable_string: str = \"API Key\"):\n",
    "    api_key = os.getenv(env_var_name)\n",
    "  \n",
    "    if api_key:\n",
    "       print(f\"{env_var_name} is present\")\n",
    "    else:\n",
    "      print(f\"{env_var_name} is NOT present, paste key at the prompt:\")\n",
    "      os.environ[env_var_name] = getpass.getpass(f\"Please enter your {human_readable_string}: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wujPjGJuoPwg"
   },
   "source": [
    "## Task 2: Environment Variables\n",
    "\n",
    "We'll want to set both our OpenAI API key and our LangSmith environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jdh8CoVWHRvs",
    "outputId": "3fa78560-393c-4ee5-b871-9886bf0d70f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is present\n",
      "TAVILY_API_KEY is present\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "check_if_env_var_is_set(\"OPENAI_API_KEY\", \"OpenAI API key\")\n",
    "check_if_env_var_is_set(\"TAVILY_API_KEY\", \"TAVILY API key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nv0glIDyHmRt",
    "outputId": "b69df90a-b4e1-4ddb-9de0-882d98b68ab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGCHAIN_API_KEY is present\n"
     ]
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE7 - Certification Challenge\"\n",
    "check_if_env_var_is_set(\"LANGCHAIN_API_KEY\", \"LangSmith API Key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBRyQmEAVzua"
   },
   "source": [
    "## Task 3: Creating our Tool Belt\n",
    "\n",
    "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
    "\n",
    "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain-community/tree/main/libs/community) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
    "\n",
    "We'll leverage:\n",
    "\n",
    "- [Tavily Search Results](https://github.com/langchain-ai/langchain-community/blob/main/libs/community/langchain_community/tools/tavily_search/tool.py)\n",
    "- [Arxiv](https://github.com/langchain-ai/langchain-community/blob/main/libs/community/langchain_community/tools/arxiv/tool.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2k6n_Dob2F46"
   },
   "source": [
    "#### ðŸ—ï¸ Activity #1:\n",
    "\n",
    "Please add the tools to use into our toolbelt.\n",
    "\n",
    "> NOTE: Each tool in our toolbelt should be a method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def tavily_studentaid_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search ONLY StudentAid.gov for official federal information: FAFSA applications, \n",
    "    federal loan forgiveness programs, federal repayment plans, eligibility requirements.\n",
    "    Use this when you need authoritative federal government information.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query for federal student aid topics\n",
    "        \n",
    "    Returns:\n",
    "        Formatted search results from StudentAid.gov\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "        \n",
    "        response = client.search(\n",
    "            query=f\"site:studentaid.gov {query}\",\n",
    "            search_depth=\"advanced\",\n",
    "            max_results=3,\n",
    "            include_answer=True,\n",
    "            include_domains=[\"studentaid.gov\"]\n",
    "        )\n",
    "        \n",
    "        result = f\"StudentAid.gov Search Results for: {query}\\n\\n\"\n",
    "        \n",
    "        if response.get('answer'):\n",
    "            result += f\"Summary: {response['answer']}\\n\\n\"\n",
    "        \n",
    "        result += \"Official Federal Information:\\n\"\n",
    "        for i, item in enumerate(response.get('results', []), 1):\n",
    "            result += f\"{i}. {item.get('title', 'No title')}\\n\"\n",
    "            result += f\"   {item.get('content', '')[:200]}...\\n\"\n",
    "            result += f\"   URL: {item.get('url', '')}\\n\\n\"\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error searching StudentAid.gov: {str(e)}\"\n",
    "\n",
    "\n",
    "def tavily_mohela_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search ONLY Mohela loan servicer for account-specific help: making payments, \n",
    "    login issues, servicer-specific repayment options, customer service contacts.\n",
    "    Use this when users have Mohela-serviced loans and need servicer-specific help.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query for Mohela servicer-specific information\n",
    "        \n",
    "    Returns:\n",
    "        Formatted search results from Mohela\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "        \n",
    "        response = client.search(\n",
    "            query=f\"site:mohela.com OR site:servicing.mohela.com {query}\",\n",
    "            search_depth=\"advanced\",\n",
    "            max_results=3,\n",
    "            include_answer=True,\n",
    "            include_domains=[\"mohela.com\", \"servicing.mohela.com\"]\n",
    "        )\n",
    "        \n",
    "        result = f\"Mohela Search Results for: {query}\\n\\n\"\n",
    "        \n",
    "        if response.get('answer'):\n",
    "            result += f\"Summary: {response['answer']}\\n\\n\"\n",
    "        \n",
    "        result += \"Mohela Servicer Information:\\n\"\n",
    "        for i, item in enumerate(response.get('results', []), 1):\n",
    "            result += f\"{i}. {item.get('title', 'No title')}\\n\"\n",
    "            result += f\"   {item.get('content', '')[:200]}...\\n\"\n",
    "            result += f\"   URL: {item.get('url', '')}\\n\\n\"\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error searching Mohela: {str(e)}\"\n",
    "\n",
    "\n",
    "def tavily_student_loan_search(query: str, source: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Compare information across BOTH federal sources and Mohela when user needs \n",
    "    comprehensive view or comparison of student loan options. Use this when users \n",
    "    want to see both federal policies and servicer-specific implementation, or when \n",
    "    they're unsure which source has the information they need.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query for student loan information requiring comparison\n",
    "        source: Optional - \"studentaid\" for StudentAid.gov only, \"mohela\" for Mohela only\n",
    "        \n",
    "    Returns:\n",
    "        Formatted search results comparing both sources\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "        \n",
    "        if source == \"studentaid\":\n",
    "            return tavily_studentaid_search(query)\n",
    "        elif source == \"mohela\":\n",
    "            return tavily_mohela_search(query)\n",
    "        else:\n",
    "            # Search both sources for comparison\n",
    "            search_query = f\"student loan {query} site:studentaid.gov OR site:mohela.com\"\n",
    "            \n",
    "            response = client.search(\n",
    "                query=search_query,\n",
    "                search_depth=\"advanced\",\n",
    "                max_results=5,\n",
    "                include_answer=True,\n",
    "                include_domains=[\"studentaid.gov\", \"mohela.com\", \"servicing.mohela.com\"]\n",
    "            )\n",
    "            \n",
    "            result = f\"Comprehensive Student Loan Search Results for: {query}\\n\\n\"\n",
    "            \n",
    "            if response.get('answer'):\n",
    "                result += f\"Overall Summary: {response['answer']}\\n\\n\"\n",
    "            \n",
    "            # Group results by domain for comparison\n",
    "            studentaid_results = []\n",
    "            mohela_results = []\n",
    "            \n",
    "            for item in response.get('results', []):\n",
    "                url = item.get('url', '')\n",
    "                if 'studentaid.gov' in url:\n",
    "                    studentaid_results.append(item)\n",
    "                elif 'mohela.com' in url:\n",
    "                    mohela_results.append(item)\n",
    "            \n",
    "            if studentaid_results:\n",
    "                result += \"ðŸ“‹ Federal Government Perspective (StudentAid.gov):\\n\"\n",
    "                for i, item in enumerate(studentaid_results[:2], 1):\n",
    "                    result += f\"{i}. {item.get('title', 'No title')}\\n\"\n",
    "                    result += f\"   {item.get('content', '')[:150]}...\\n\"\n",
    "                    result += f\"   Source: {item.get('url', '')}\\n\\n\"\n",
    "            \n",
    "            if mohela_results:\n",
    "                result += \"ðŸ¢ Loan Servicer Perspective (Mohela):\\n\"\n",
    "                for i, item in enumerate(mohela_results[:2], 1):\n",
    "                    result += f\"{i}. {item.get('title', 'No title')}\\n\"\n",
    "                    result += f\"   {item.get('content', '')[:150]}...\\n\"\n",
    "                    result += f\"   Source: {item.get('url', '')}\\n\\n\"\n",
    "            \n",
    "            if not studentaid_results and not mohela_results:\n",
    "                result += \"No specific results found from StudentAid.gov or Mohela. Consider using general web search.\\n\"\n",
    "            \n",
    "            return result\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error searching student loan information: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lAxaSvlfIeOg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3124/679774791.py:6: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tavily_tool = TavilySearchResults(max_results=5)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "\n",
    "from langchain.tools import Tool\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "# tool_belt = [\n",
    "#     tavily_tool,\n",
    "#     ArxivQueryRun(),\n",
    "# ]\n",
    "\n",
    "tool_belt = [\n",
    "    tavily_tool,\n",
    "    Tool(\n",
    "        name=\"StudentAid_Federal_Search\",\n",
    "        description=\"Search ONLY StudentAid.gov for official federal information: FAFSA applications, federal loan forgiveness programs, federal repayment plans, eligibility requirements\",\n",
    "        func=tavily_studentaid_search\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Mohela_Servicer_Search\", \n",
    "        description=\"Search ONLY Mohela loan servicer for account-specific help: making payments, login issues, servicer-specific repayment options, customer service contacts\",\n",
    "        func=tavily_mohela_search\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Student_Loan_Comparison_Search\",\n",
    "        description=\"Compare information across BOTH federal sources and Mohela when user needs comprehensive view or comparison of student loan options\",\n",
    "        func=tavily_student_loan_search\n",
    "    ),\n",
    "    ArxivQueryRun(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VI-C669ZYVI5"
   },
   "source": [
    "### Model\n",
    "\n",
    "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
    "\n",
    "- OpenAI's GPT-3.5 and GPT-4\n",
    "- Anthropic's Claude\n",
    "- Google's Gemini\n",
    "\n",
    "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QkNS8rNZJs4z"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ugkj3GzuZpQv"
   },
   "source": [
    "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4OdMqFafZ_0V"
   },
   "outputs": [],
   "source": [
    "model = model.bind_tools(tool_belt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mxL9b_NZKUdL"
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "  messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "91flJWtZLUrl"
   },
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "def call_model(state):\n",
    "  messages = state[\"messages\"]\n",
    "  response = model.invoke(messages)\n",
    "  return {\"messages\" : [response]}\n",
    "\n",
    "tool_node = ToolNode(tool_belt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bwR7MgWj3Wg"
   },
   "source": [
    "Now we have two total nodes. We have:\n",
    "\n",
    "- `call_model` is a node that will...well...call the model\n",
    "- `tool_node` is a node which can call a tool\n",
    "\n",
    "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_vF4_lgtmQNo",
    "outputId": "a4384377-8f7a-415f-be1b-fee6169cb101"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7f7419a638c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "uncompiled_graph = StateGraph(AgentState)\n",
    "\n",
    "uncompiled_graph.add_node(\"agent\", call_model)\n",
    "uncompiled_graph.add_node(\"action\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaXHpPeSnOWC"
   },
   "source": [
    "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGCbaYqRnmiw",
    "outputId": "5351807c-2ac7-4316-a3a3-878abeacd114"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7f7419a638c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncompiled_graph.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BZgb81VQf9o",
    "outputId": "73a07c15-5f0b-40f2-b033-38b57d056dd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7f7419a638c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def should_continue(state):\n",
    "  last_message = state[\"messages\"][-1]\n",
    "\n",
    "  if last_message.tool_calls:\n",
    "    return \"action\"\n",
    "\n",
    "  return END\n",
    "\n",
    "uncompiled_graph.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UvcgbHf1rIXZ",
    "outputId": "45d4bdd6-d6bb-4a1d-bb79-cad43c130bf2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7f7419a638c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncompiled_graph.add_edge(\"action\", \"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "zt9-KS8DpzNx"
   },
   "outputs": [],
   "source": [
    "simple_agent_graph = uncompiled_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEYcTShCsPaa"
   },
   "source": [
    "## Using Our Graph\n",
    "\n",
    "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
    "\n",
    "Let's try out a few examples to see how it fairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tool_calls_parser\n",
    "importlib.reload(tool_calls_parser)\n",
    "from tool_calls_parser import parse_logs, print_formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qn4n37PQRPII",
    "outputId": "5eeedfae-089d-496e-e71f-071939fa5832",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving update from node: 'agent'\n",
      "\n",
      "\n",
      "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_8frtUcW0lq993ybwnysEfS54', 'function': {'arguments': '{\"query\":\"current captain of the Winnipeg Jets\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 310, 'total_tokens': 333, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-Bzaw7dpmYekX32fa8yu0Da2sxSakN', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--e756eedf-fb70-4cba-b80d-c0f45d6dd450-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current captain of the Winnipeg Jets'}, 'id': 'call_8frtUcW0lq993ybwnysEfS54', 'type': 'tool_call'}], usage_metadata={'input_tokens': 310, 'output_tokens': 23, 'total_tokens': 333, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'action'\n",
      "Tool Used: tavily_search_results_json\n",
      "=== PARSING RESULTS ===\n",
      "Tool calls: 0\n",
      "Tool results: 1\n",
      "Tools used: ['tavily_search_results_json']\n",
      "\n",
      "=== TOOL CALLS ===\n",
      "\n",
      "=== TOOL RESULTS ===\n",
      "1. tavily_search_results_json -> [{\"title\": \"Adam Lowry named Jets captain | Winnipeg Jets - NHL.com\", \"url\": \"https://www.nhl.com/je...\n",
      "\n",
      "\n",
      "[ToolMessage(content='[{\"title\": \"Adam Lowry named Jets captain | Winnipeg Jets - NHL.com\", \"url\": \"https://www.nhl.com/jets/news/adam-lowry-named-jets-captain\", \"content\": \"That honour was given to Winnipeg Jet forward Adam Lowry officially Tuesday morning as he becomes the third captain in franchise history since the team moved here from Atlanta. He follows Andrew Ladd and Blake Wheeler who served as captain for five and six years respectively.\\\\n\\\\nÃ¢\\x80\\x9cWhen I found out, I was pretty excited, almost a little speechless. ItÃ¢\\x80\\x99s something growing up you kind of can dream about and something that seems almost unattainable,Ã¢\\x80\\x9d said Lowry. [...] Ã¢\\x80\\x9cHeÃ¢\\x80\\x99s a true professional, he has total respect from every player on the team, every player around the league and certainly from the coaching staff as well. We just feel at this point itÃ¢\\x80\\x99s the right time to name Adam as our captain.Ã¢\\x80\\x9d [...] Ã¢\\x80\\x9cGetting to be a captain of a Canadian NHL team is pretty special and something IÃ¢\\x80\\x99m really looking forward too.Ã¢\\x80\\x9d\", \"score\": 0.8449346}, {\"title\": \"Lowry named Jets captain, replaces Wheeler | NHL.com\", \"url\": \"https://www.nhl.com/news/adam-lowry-named-winnipeg-captain\", \"content\": \"NHL logo\\\\nNHL logo\\\\n\\\\n# Lowry named Jets captain, replaces Wheeler\\\\n\\\\n30-year-old forward entering 10th season with Winnipeg\\\\n\\\\nLowry_Jets\\\\n\\\\nAdam Lowry was named captain of the Winnipeg Jets on Tuesday.\\\\n\\\\nThe 30-year-old forward was selected by the Jets in the third round (No. 67) of the 2011 NHL Draft and has played his entire nine-season NHL career with Winnipeg. [...] Lowry replaces Blake Wheeler, who was removed as captain Sept. 16, 2022, and signed with the New York Rangers after having his contract bought out this offseason. The Jets opted for three alternate captains last season; Lowry, forward Mark Scheifele and defenseman Josh Morrissey. Coach Rick Bowness said Scheifele and Morrissey will remain alternate captains. [...] Lowry said he\\'s learned from the captains he\\'s played with in Winnipeg, Wheeler and Andrew Ladd, and believes the important thing is staying true to the player he\\'s always been.\", \"score\": 0.84027237}, {\"title\": \"Jets sign Toews for leadership, add Nyquist, Pearson for depth\", \"url\": \"https://www.nhl.com/news/winnipeg-jets-roster-changes-for-2025-26-season\", \"content\": \"games for the Vegas Golden Knights. Pearson could find a role in Winnipeg\\'s middle six. The need there will be magnified. Jets captain and center Adam Lowry is expected to miss the start of the season after having hip surgery May 27, with a 5-6 month timeline for recovery. [...] Nikolaj Ehlers, F: The 29-year-old signed a six-year contract with the Carolina Hurricanes on July 3 after he had 63 points (24 goals, 39 assists) in 69 games last season and seven points (five goals, two assists) in eight Stanley Cup Playoff games. Ehlers had 520 points (225 goals, 295 assists) in 674 games through 10 seasons with the Jets. Ã¢\\x80Â¦ Mason Appleton, F: The 29-year-old signed a two-year contract with the Detroit Red Wings on July 2. He had 22 points (10 goals, 12 assists) in 71 games [...] ## EDGE stat to watch\\\\n\\\\nThe Jets ranked highly in high-danger goals (144; third) and long-range goals (29; first) last season. Winnipeg had players rank among the NHL leaders in goals from each major shot location region: Center Mark Scheifele was tied for fourth in the NHL in high-danger goals (24), forward Kyle Connor tied for fourth in midrange goals (17) and defensemen Neal Pionk and Josh Morrissey tied for fourth in long-range goals (six each). -- Troy Perlowitz\", \"score\": 0.78014404}, {\"title\": \"Team Captains of Winnipeg Jets - Elite Prospects\", \"url\": \"https://www.eliteprospects.com/team/9966/winnipeg-jets/captaincy-history\", \"content\": \"| Season | League | â€œCâ€ Captain(s) | â€œAâ€ Alternate Captain(s) |\\\\n| --- | --- | --- | --- |\\\\n| 2025-2026  2025-26 | NHL | Adam Lowry |  |\\\\n| 2024-2025  2024-25 | NHL | Adam Lowry | Josh Morrissey  Neal Pionk  Mark Scheifele |\\\\n| 2023-2024  2023-24 | NHL | Adam Lowry | Josh Morrissey  Mark Scheifele |\\\\n| 2022-2023  2022-23 | NHL |  | Adam Lowry  Josh Morrissey  Mark Scheifele |\\\\n| 2021-2022  2021-22 | NHL | Blake Wheeler | Josh Morrissey  Mark Scheifele | [...] | 2020-2021  2020-21 | NHL | Blake Wheeler | Josh Morrissey  Mark Scheifele |\\\\n| 2019-2020  2019-20 | NHL | Blake Wheeler | Mark Scheifele  Bryan Little  Josh Morrissey |\\\\n| 2018-2019  2018-19 | NHL | Blake Wheeler | Mark Scheifele  Bryan Little  Dustin Byfuglien |\\\\n| 2017-2018  2017-18 | NHL | Blake Wheeler | Dustin Byfuglien  Bryan Little  Mark Scheifele |\\\\n| 2016-2017  2016-17 | NHL | Blake Wheeler | Dustin Byfuglien  Mark Scheifele | [...] | Name | League |\\\\n| --- | --- |\\\\n| Canada Noland Lillington (D) | NSRJHL |\\\\n| Sweden Marcus Eriksson (F) | EIHL |\\\\n| USA Alex Limoges (F) | KHL |\\\\n| Sweden William Worge KreÃ¼ (D) | Liiga |\\\\n| USA Kyle McDonough (F) | Norway |\\\\n| Sweden Emil Kruse (G) | HockeyAllsvenskan |\\\\n| Finland Aatos Koivu (F) | Liiga |\\\\n| USA Brandon Fortunato (D) | AlpsHL |\\\\n| Canada Gavin McKenna (F) | NCAA |\\\\n| Canada Devon Levi (G) | NHL |\\\\n| Canada Tim Daly (D) | Denmark |\\\\n| Canada Jason Smith (D) | NHL |\", \"score\": 0.7663815}, {\"title\": \"Winnipeg Jets - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Winnipeg_Jets\", \"content\": \"2025â€“26 Winnipeg Jets season. Conference, Western. Division, Central. Founded ... ^ \\\\\"Adam Lowry named Jets captain\\\\\". NHL.com. September 12, 2023. Retrieved\", \"score\": 0.76194656}]', name='tavily_search_results_json', id='03e97198-bc1d-4f49-896a-538a0600919a', tool_call_id='call_8frtUcW0lq993ybwnysEfS54', artifact={'query': 'current captain of the Winnipeg Jets', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.nhl.com/jets/news/adam-lowry-named-jets-captain', 'title': 'Adam Lowry named Jets captain | Winnipeg Jets - NHL.com', 'content': 'That honour was given to Winnipeg Jet forward Adam Lowry officially Tuesday morning as he becomes the third captain in franchise history since the team moved here from Atlanta. He follows Andrew Ladd and Blake Wheeler who served as captain for five and six years respectively.\\n\\nÃ¢\\x80\\x9cWhen I found out, I was pretty excited, almost a little speechless. ItÃ¢\\x80\\x99s something growing up you kind of can dream about and something that seems almost unattainable,Ã¢\\x80\\x9d said Lowry. [...] Ã¢\\x80\\x9cHeÃ¢\\x80\\x99s a true professional, he has total respect from every player on the team, every player around the league and certainly from the coaching staff as well. We just feel at this point itÃ¢\\x80\\x99s the right time to name Adam as our captain.Ã¢\\x80\\x9d [...] Ã¢\\x80\\x9cGetting to be a captain of a Canadian NHL team is pretty special and something IÃ¢\\x80\\x99m really looking forward too.Ã¢\\x80\\x9d', 'score': 0.8449346, 'raw_content': None}, {'url': 'https://www.nhl.com/news/adam-lowry-named-winnipeg-captain', 'title': 'Lowry named Jets captain, replaces Wheeler | NHL.com', 'content': \"NHL logo\\nNHL logo\\n\\n# Lowry named Jets captain, replaces Wheeler\\n\\n30-year-old forward entering 10th season with Winnipeg\\n\\nLowry_Jets\\n\\nAdam Lowry was named captain of the Winnipeg Jets on Tuesday.\\n\\nThe 30-year-old forward was selected by the Jets in the third round (No. 67) of the 2011 NHL Draft and has played his entire nine-season NHL career with Winnipeg. [...] Lowry replaces Blake Wheeler, who was removed as captain Sept. 16, 2022, and signed with the New York Rangers after having his contract bought out this offseason. The Jets opted for three alternate captains last season; Lowry, forward Mark Scheifele and defenseman Josh Morrissey. Coach Rick Bowness said Scheifele and Morrissey will remain alternate captains. [...] Lowry said he's learned from the captains he's played with in Winnipeg, Wheeler and Andrew Ladd, and believes the important thing is staying true to the player he's always been.\", 'score': 0.84027237, 'raw_content': None}, {'url': 'https://www.nhl.com/news/winnipeg-jets-roster-changes-for-2025-26-season', 'title': 'Jets sign Toews for leadership, add Nyquist, Pearson for depth', 'content': \"games for the Vegas Golden Knights. Pearson could find a role in Winnipeg's middle six. The need there will be magnified. Jets captain and center Adam Lowry is expected to miss the start of the season after having hip surgery May 27, with a 5-6 month timeline for recovery. [...] Nikolaj Ehlers, F: The 29-year-old signed a six-year contract with the Carolina Hurricanes on July 3 after he had 63 points (24 goals, 39 assists) in 69 games last season and seven points (five goals, two assists) in eight Stanley Cup Playoff games. Ehlers had 520 points (225 goals, 295 assists) in 674 games through 10 seasons with the Jets. Ã¢\\x80Â¦ Mason Appleton, F: The 29-year-old signed a two-year contract with the Detroit Red Wings on July 2. He had 22 points (10 goals, 12 assists) in 71 games [...] ## EDGE stat to watch\\n\\nThe Jets ranked highly in high-danger goals (144; third) and long-range goals (29; first) last season. Winnipeg had players rank among the NHL leaders in goals from each major shot location region: Center Mark Scheifele was tied for fourth in the NHL in high-danger goals (24), forward Kyle Connor tied for fourth in midrange goals (17) and defensemen Neal Pionk and Josh Morrissey tied for fourth in long-range goals (six each). -- Troy Perlowitz\", 'score': 0.78014404, 'raw_content': None}, {'url': 'https://www.eliteprospects.com/team/9966/winnipeg-jets/captaincy-history', 'title': 'Team Captains of Winnipeg Jets - Elite Prospects', 'content': '| Season | League | â€œCâ€ Captain(s) | â€œAâ€ Alternate Captain(s) |\\n| --- | --- | --- | --- |\\n| 2025-2026  2025-26 | NHL | Adam Lowry |  |\\n| 2024-2025  2024-25 | NHL | Adam Lowry | Josh Morrissey  Neal Pionk  Mark Scheifele |\\n| 2023-2024  2023-24 | NHL | Adam Lowry | Josh Morrissey  Mark Scheifele |\\n| 2022-2023  2022-23 | NHL |  | Adam Lowry  Josh Morrissey  Mark Scheifele |\\n| 2021-2022  2021-22 | NHL | Blake Wheeler | Josh Morrissey  Mark Scheifele | [...] | 2020-2021  2020-21 | NHL | Blake Wheeler | Josh Morrissey  Mark Scheifele |\\n| 2019-2020  2019-20 | NHL | Blake Wheeler | Mark Scheifele  Bryan Little  Josh Morrissey |\\n| 2018-2019  2018-19 | NHL | Blake Wheeler | Mark Scheifele  Bryan Little  Dustin Byfuglien |\\n| 2017-2018  2017-18 | NHL | Blake Wheeler | Dustin Byfuglien  Bryan Little  Mark Scheifele |\\n| 2016-2017  2016-17 | NHL | Blake Wheeler | Dustin Byfuglien  Mark Scheifele | [...] | Name | League |\\n| --- | --- |\\n| Canada Noland Lillington (D) | NSRJHL |\\n| Sweden Marcus Eriksson (F) | EIHL |\\n| USA Alex Limoges (F) | KHL |\\n| Sweden William Worge KreÃ¼ (D) | Liiga |\\n| USA Kyle McDonough (F) | Norway |\\n| Sweden Emil Kruse (G) | HockeyAllsvenskan |\\n| Finland Aatos Koivu (F) | Liiga |\\n| USA Brandon Fortunato (D) | AlpsHL |\\n| Canada Gavin McKenna (F) | NCAA |\\n| Canada Devon Levi (G) | NHL |\\n| Canada Tim Daly (D) | Denmark |\\n| Canada Jason Smith (D) | NHL |', 'score': 0.7663815, 'raw_content': None}, {'url': 'https://en.wikipedia.org/wiki/Winnipeg_Jets', 'title': 'Winnipeg Jets - Wikipedia', 'content': '2025â€“26 Winnipeg Jets season. Conference, Western. Division, Central. Founded ... ^ \"Adam Lowry named Jets captain\". NHL.com. September 12, 2023. Retrieved', 'score': 0.76194656, 'raw_content': None}], 'response_time': 3.62})]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'agent'\n",
      "\n",
      "\n",
      "[AIMessage(content='The current captain of the Winnipeg Jets is Adam Lowry.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 1898, 'total_tokens': 1911, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-BzawCxxSy6jbmrM3alzCH20OhxUxM', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--44a6a6d1-0df3-48df-b060-ebe5f1dc94ef-0', usage_metadata={'input_tokens': 1898, 'output_tokens': 13, 'total_tokens': 1911, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\" : [HumanMessage(content=\"Who is the current captain of the Winnipeg Jets?\")]}\n",
    "\n",
    "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: '{node}'\")\n",
    "        if node == \"action\":\n",
    "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
    "          print_formatted_results(parse_logs(str(values[\"messages\"])))\n",
    "        print(\"\\n\")\n",
    "        print(values[\"messages\"])\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBHnUtLSscRr"
   },
   "source": [
    "Let's look at what happened:\n",
    "\n",
    "1. Our state object was populated with our request\n",
    "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
    "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
    "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
    "5. The agent node added a response to the state object and passed it along the conditional edge\n",
    "6. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
    "\n",
    "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afv2BuEsV5JG",
    "outputId": "ff009536-d281-4a56-c126-9cd245352bfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_qWCsMXce5OBaDgRwDfRWOJ8N', 'function': {'arguments': '{\"query\": \"QLoRA\"}', 'name': 'arxiv'}, 'type': 'function'}, {'id': 'call_iBNDr3tUBq9qzJwO1Qd4qsd6', 'function': {'arguments': '{\"query\": \"latest Tweet of each author of the QLoRA paper\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 60, 'prompt_tokens': 326, 'total_tokens': 386, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-BzawDGRfjgfJ5dONfvMpErVGhKQGa', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--c2f9fe08-d9b1-4123-a1f8-5353c0007f3f-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'QLoRA'}, 'id': 'call_qWCsMXce5OBaDgRwDfRWOJ8N', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'latest Tweet of each author of the QLoRA paper'}, 'id': 'call_iBNDr3tUBq9qzJwO1Qd4qsd6', 'type': 'tool_call'}], usage_metadata={'input_tokens': 326, 'output_tokens': 60, 'total_tokens': 386, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'action'\n",
      "Tool Used: arxiv\n",
      "=== PARSING RESULTS ===\n",
      "Tool calls: 0\n",
      "Tool results: 2\n",
      "Tools used: ['tavily_search_results_json', 'arxiv']\n",
      "\n",
      "=== TOOL CALLS ===\n",
      "\n",
      "=== TOOL RESULTS ===\n",
      "1. arxiv -> Published: 2023-05-23 Title: QLoRA: Efficient Finetuning of Quantized LLMs Authors: Tim Dettmers, Ar...\n",
      "2. tavily_search_results_json -> [{\"title\": \"LLM Research Insights: Instruction Masking and New LoRA ...\", \"url\": \"https://sebastianr...\n",
      "[ToolMessage(content='Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2025-02-05\\nTitle: Resource-Efficient & Effective Code Summarization\\nAuthors: Saima Afrin, Joseph Call, Khai-Nguyen Nguyen, Oscar Chaparro, Antonio Mastropaolo\\nSummary: Code Language Models (CLMs) have demonstrated high effectiveness in\\nautomating software engineering tasks such as bug fixing, code generation, and\\ncode documentation. This ', name='arxiv', id='e4f149a3-c4ca-43c4-a81f-6efc0fdce0b8', tool_call_id='call_qWCsMXce5OBaDgRwDfRWOJ8N'), ToolMessage(content='[{\"title\": \"LLM Research Insights: Instruction Masking and New LoRA ...\", \"url\": \"https://sebastianraschka.com/blog/2024/llm-research-insights-instruction.html\", \"content\": \"`` LoRA Learns Less and Forgets Less by Biderman, Ortiz, Portes, et al. (15 May), \\\\n\\\\n``\\\\n\\\\nRLHF Workflow: From Reward Modeling to Online RLHF by Dong, Xiong, Pang, et al. (13 May), \\\\n\\\\nPHUDGE: Phi-3 as Scalable Judge by Deshwal and Chawla (12 May), \\\\n\\\\nValue Augmented Sampling for Language Model Alignment and Personalization by Han, Shenfeld, Srivastava, et al.(10 May), \\\\n\\\\n`` Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations? by Gekhman, Yona, Aharoni, et al. (9 May), \\\\n\\\\n`` [...] `` Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models by Land and Bartolo (8 May), \\\\n\\\\n``\\\\n\\\\n`` DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model by Liu, Feng, Wang, et al. (8 May), \\\\n\\\\n``\\\\n\\\\nYou Only Cache Once: Decoder-Decoder Architectures for Language Models by Sun, Dong, Zhu, et al. (8 May), \\\\n\\\\n`` xLSTM: Extended Long Short-Term Memory by Beck, Poeppel, Spanring, et al. (7 May), \\\\n\\\\n`` [...] SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization by Guo, Chen, Tang, and Wang (19 May), \\\\n\\\\nTowards Modular LLMs by Building and Reusing a Library of LoRAs by Ostapenko, Su, Ponti, et al. (17 May), \\\\n\\\\nChameleon: Mixed-Modal Early-Fusion Foundation Models by unknown authors at Meta AI (16 May), \\\\n\\\\nXmodel-VLM: A Simple Baseline for Multimodal Vision Language Model by Xu, Liu, He, et al. (15 May),\", \"score\": 0.49011418}, {\"title\": \"[PDF] QLORA: Efficient Finetuning of Quantized LLMs - OpenReview\", \"url\": \"https://openreview.net/pdf?id=OUIFPHEgJU\", \"content\": \"Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\\\\n Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.\\\\n Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S. [...] 11  E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\\\\n J. Fu, S.-K. Ng, Z. Jiang, and P. Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166, 2023.\\\\n X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, and D. Song. Koala: A dialogue model for academic research. Blog post, April 2023. URL \\\\nedu/blog/2023/04/03/koala/. [...] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingfaceâ€™s transformers: State-of-the-art natural language processing.\\\\narXiv preprint arXiv:1910.03771, 2019.\\\\n M. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt. Stable and low-precision training for large-scale vision-language models. arXiv preprint arXiv:2304.13013, 2023.\", \"score\": 0.45217365}, {\"title\": \"Brief Review â€” QLoRA: Efficient Finetuning of Quantized LLMs\", \"url\": \"https://sh-tsang.medium.com/brief-review-qlora-efficient-finetuning-of-quantized-llms-6f20a7255701\", \"content\": \"--\\\\n\\\\nSik-Ho Tsang\\\\nSik-Ho Tsang\\\\n\\\\n## Written by Sik-Ho Tsang\\\\n\\\\nPhD, Researcher. I share what I learn. :) Linktree:  for Twitter, LinkedIn, etc.\\\\n\\\\n## No responses yet\\\\n\\\\nHelp\\\\n\\\\nStatus\\\\n\\\\nAbout\\\\n\\\\nCareers\\\\n\\\\nPress\\\\n\\\\nBlog\\\\n\\\\nPrivacy\\\\n\\\\nRules\\\\n\\\\nTerms\\\\n\\\\nText to speech [...] Sign up\\\\n\\\\nSign in\\\\n\\\\nSign up\\\\n\\\\nSign in\\\\n\\\\n# Brief Review â€” QLoRA: Efficient Finetuning of Quantized LLMs\\\\n\\\\n## QLoRA, LoRA With Quantization. A Model Family, Guanaco, Is Proposed.\\\\n\\\\nSik-Ho Tsang\\\\n\\\\n--\\\\n\\\\nListen\\\\n\\\\nShare\\\\n\\\\nQLoRA: Efficient Finetuning of Quantized LLMs QLoRA, Guanaco, by University of Washington  \\\\n2023 NeurIPS, Over 650 Citations (\\\\n\\\\nSik-Ho Tsang\\\\n\\\\n @ Medium) [...] LM Tuning / Prompting  \\\\n2020 â€¦ 2023 [LIMA] [SELF-INTRUCT] [Self-Consistency] [Med-PaLM 2] 2024 [LLaMA-Adapter]  \\\\n==== My Other Paper Readings Are Also Over Here ====\\\\n\\\\n# Outline\\\\n\\\\n# 1. Background\\\\n\\\\n## 1.1. Block-wise k-bit Quantization\\\\n\\\\n## 1.2. Low-rank Adapter (LoRA) Finetuning\\\\n\\\\n# 2. QLoRA Finetuning\\\\n\\\\n## 2.1. 4-bit NormalFloat Quantization\\\\n\\\\n## 2.2. Double Quantization (DQ)\\\\n\\\\n## 2.3. Paged Optimizers\\\\n\\\\n## 2.4. QLoRA\", \"score\": 0.445506}, {\"title\": \"[PDF] QLORA: Efficient Finetuning of Quantized LLMs - arXiv\", \"url\": \"https://arxiv.org/pdf/2305.14314\", \"content\": \"S. H. Bach, V. Sanh, Z.-X. Yong, A. Webson, C. Raffel, N. V. Nayak, A. Sharma, T. Kim, M. S.\\\\nBari, T. Fevry, et al. Promptsource: An integrated development environment and repository for natural language prompts. arXiv preprint arXiv:2202.01279, 2022.\\\\n Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [...] S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. Oâ€™Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373, 2023.\\\\n R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models.\\\\narXiv preprint arXiv:2108.07258, 2021. [...] 16 References  S. An, Y. Li, Z. Lin, Q. Liu, B. Chen, Q. Fu, W. Chen, N. Zheng, and J.-G. Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. arXiv preprint arXiv:2203.03131, 2022.\\\\n A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.\", \"score\": 0.43036336}, {\"title\": \"Mastering QLoRa : A Deep Dive into 4-Bit Quantization and LoRa ...\", \"url\": \"https://manalelaidouni.github.io/4Bit-Quantization-Models-QLoRa.html\", \"content\": \"The Low-Rank Adaptation paper is influenced by ideas in the Li et al. and Aghajanyan et al. papers that show that language models are over-parameterized and have a very low intrinsic dimension that can be effectively represented using fewer dimensions.\\\\n\\\\nIt turns out that LLMs have a low dimension rank structure, meaning that important features and patterns in the model can be represented using significantly fewer dimensions, efficitively reducing the model complexity. [...] Donâ€™t take my word for it, here is the empirical results from the paper:\\\\n\\\\nThe paper uses GPT-3 175B LLM as an example. Using LoRa finetuning, the number of trainable parameters was reduced by 10,000 times compared to full finetuning. Additionally, VRAM consumption during finetuning decreased from 1.2TB to 350GB, while checkpoint size was downsized from 350GB to 35MB. This translated into a 25% speedup on the finetuning process. [...] At the normalization step of block-wise quantization, we mentioned that the scaling factors, which are the absolute maximum values, are stored for dequantization. These scaling factors are essentially quantization constants that require additional space to be stored. To address this, the QLoRa paper suggests a solution to further reduce memory storage, which is to quantize the quantization constants themselves using block-wise quantization. This way, we end up with both quantized weights and\", \"score\": 0.19859245}]', name='tavily_search_results_json', id='3cba8974-c154-4f9b-8476-c262ac3d8d2e', tool_call_id='call_iBNDr3tUBq9qzJwO1Qd4qsd6', artifact={'query': 'latest Tweet of each author of the QLoRA paper', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://sebastianraschka.com/blog/2024/llm-research-insights-instruction.html', 'title': 'LLM Research Insights: Instruction Masking and New LoRA ...', 'content': '`` LoRA Learns Less and Forgets Less by Biderman, Ortiz, Portes, et al. (15 May), \\n\\n``\\n\\nRLHF Workflow: From Reward Modeling to Online RLHF by Dong, Xiong, Pang, et al. (13 May), \\n\\nPHUDGE: Phi-3 as Scalable Judge by Deshwal and Chawla (12 May), \\n\\nValue Augmented Sampling for Language Model Alignment and Personalization by Han, Shenfeld, Srivastava, et al.(10 May), \\n\\n`` Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations? by Gekhman, Yona, Aharoni, et al. (9 May), \\n\\n`` [...] `` Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models by Land and Bartolo (8 May), \\n\\n``\\n\\n`` DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model by Liu, Feng, Wang, et al. (8 May), \\n\\n``\\n\\nYou Only Cache Once: Decoder-Decoder Architectures for Language Models by Sun, Dong, Zhu, et al. (8 May), \\n\\n`` xLSTM: Extended Long Short-Term Memory by Beck, Poeppel, Spanring, et al. (7 May), \\n\\n`` [...] SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization by Guo, Chen, Tang, and Wang (19 May), \\n\\nTowards Modular LLMs by Building and Reusing a Library of LoRAs by Ostapenko, Su, Ponti, et al. (17 May), \\n\\nChameleon: Mixed-Modal Early-Fusion Foundation Models by unknown authors at Meta AI (16 May), \\n\\nXmodel-VLM: A Simple Baseline for Multimodal Vision Language Model by Xu, Liu, He, et al. (15 May),', 'score': 0.49011418, 'raw_content': None}, {'url': 'https://openreview.net/pdf?id=OUIFPHEgJU', 'title': '[PDF] QLORA: Efficient Finetuning of Quantized LLMs - OpenReview', 'content': 'Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\\n Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.\\n Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S. [...] 11  E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\\n J. Fu, S.-K. Ng, Z. Jiang, and P. Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166, 2023.\\n X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, and D. Song. Koala: A dialogue model for academic research. Blog post, April 2023. URL \\nedu/blog/2023/04/03/koala/. [...] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingfaceâ€™s transformers: State-of-the-art natural language processing.\\narXiv preprint arXiv:1910.03771, 2019.\\n M. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt. Stable and low-precision training for large-scale vision-language models. arXiv preprint arXiv:2304.13013, 2023.', 'score': 0.45217365, 'raw_content': None}, {'url': 'https://sh-tsang.medium.com/brief-review-qlora-efficient-finetuning-of-quantized-llms-6f20a7255701', 'title': 'Brief Review â€” QLoRA: Efficient Finetuning of Quantized LLMs', 'content': '--\\n\\nSik-Ho Tsang\\nSik-Ho Tsang\\n\\n## Written by Sik-Ho Tsang\\n\\nPhD, Researcher. I share what I learn. :) Linktree:  for Twitter, LinkedIn, etc.\\n\\n## No responses yet\\n\\nHelp\\n\\nStatus\\n\\nAbout\\n\\nCareers\\n\\nPress\\n\\nBlog\\n\\nPrivacy\\n\\nRules\\n\\nTerms\\n\\nText to speech [...] Sign up\\n\\nSign in\\n\\nSign up\\n\\nSign in\\n\\n# Brief Review â€” QLoRA: Efficient Finetuning of Quantized LLMs\\n\\n## QLoRA, LoRA With Quantization. A Model Family, Guanaco, Is Proposed.\\n\\nSik-Ho Tsang\\n\\n--\\n\\nListen\\n\\nShare\\n\\nQLoRA: Efficient Finetuning of Quantized LLMs QLoRA, Guanaco, by University of Washington  \\n2023 NeurIPS, Over 650 Citations (\\n\\nSik-Ho Tsang\\n\\n @ Medium) [...] LM Tuning / Prompting  \\n2020 â€¦ 2023 [LIMA] [SELF-INTRUCT] [Self-Consistency] [Med-PaLM 2] 2024 [LLaMA-Adapter]  \\n==== My Other Paper Readings Are Also Over Here ====\\n\\n# Outline\\n\\n# 1. Background\\n\\n## 1.1. Block-wise k-bit Quantization\\n\\n## 1.2. Low-rank Adapter (LoRA) Finetuning\\n\\n# 2. QLoRA Finetuning\\n\\n## 2.1. 4-bit NormalFloat Quantization\\n\\n## 2.2. Double Quantization (DQ)\\n\\n## 2.3. Paged Optimizers\\n\\n## 2.4. QLoRA', 'score': 0.445506, 'raw_content': None}, {'url': 'https://arxiv.org/pdf/2305.14314', 'title': '[PDF] QLORA: Efficient Finetuning of Quantized LLMs - arXiv', 'content': 'S. H. Bach, V. Sanh, Z.-X. Yong, A. Webson, C. Raffel, N. V. Nayak, A. Sharma, T. Kim, M. S.\\nBari, T. Fevry, et al. Promptsource: An integrated development environment and repository for natural language prompts. arXiv preprint arXiv:2202.01279, 2022.\\n Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [...] S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. Oâ€™Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373, 2023.\\n R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models.\\narXiv preprint arXiv:2108.07258, 2021. [...] 16 References  S. An, Y. Li, Z. Lin, Q. Liu, B. Chen, Q. Fu, W. Chen, N. Zheng, and J.-G. Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. arXiv preprint arXiv:2203.03131, 2022.\\n A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.', 'score': 0.43036336, 'raw_content': None}, {'url': 'https://manalelaidouni.github.io/4Bit-Quantization-Models-QLoRa.html', 'title': 'Mastering QLoRa : A Deep Dive into 4-Bit Quantization and LoRa ...', 'content': 'The Low-Rank Adaptation paper is influenced by ideas in the Li et al. and Aghajanyan et al. papers that show that language models are over-parameterized and have a very low intrinsic dimension that can be effectively represented using fewer dimensions.\\n\\nIt turns out that LLMs have a low dimension rank structure, meaning that important features and patterns in the model can be represented using significantly fewer dimensions, efficitively reducing the model complexity. [...] Donâ€™t take my word for it, here is the empirical results from the paper:\\n\\nThe paper uses GPT-3 175B LLM as an example. Using LoRa finetuning, the number of trainable parameters was reduced by 10,000 times compared to full finetuning. Additionally, VRAM consumption during finetuning decreased from 1.2TB to 350GB, while checkpoint size was downsized from 350GB to 35MB. This translated into a 25% speedup on the finetuning process. [...] At the normalization step of block-wise quantization, we mentioned that the scaling factors, which are the absolute maximum values, are stored for dequantization. These scaling factors are essentially quantization constants that require additional space to be stored. To address this, the QLoRa paper suggests a solution to further reduce memory storage, which is to quantize the quantization constants themselves using block-wise quantization. This way, we end up with both quantized weights and', 'score': 0.19859245, 'raw_content': None}], 'response_time': 3.84})]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='I found the QLoRA paper on arXiv titled \"QLoRA: Efficient Finetuning of Quantized LLMs\" published on May 23, 2023, authored by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. \\n\\nAdditionally, I retrieved some recent tweets related to the authors and their work, but I couldn\\'t find specific latest tweets for each author directly from the search results. Would you like me to try to find their latest tweets individually or provide more details on the paper?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 117, 'prompt_tokens': 3799, 'total_tokens': 3916, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-BzawJzK5oS1kkEbbG5frmfNWJvjvn', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--0951c120-48d2-4b46-b09d-10ae2a10e72c-0', usage_metadata={'input_tokens': 3799, 'output_tokens': 117, 'total_tokens': 3916, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the QLoRA paper, then search each of the authors to find out their latest Tweet using Tavily!\")]}\n",
    "\n",
    "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: '{node}'\")\n",
    "        if node == \"action\":\n",
    "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
    "          print_formatted_results(parse_logs(str(values[\"messages\"])))\n",
    "        print(values[\"messages\"])\n",
    "\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_nERYrnrUONQ7mP4q02u5anD6', 'function': {'arguments': '{\"__arg1\":\"Aidvantage borrower complaint\"}', 'name': 'StudentAid_Federal_Search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 314, 'total_tokens': 337, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-BzawLOqSQoKXgfKMZaYBh9LvXqMZS', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--4b064091-0a2c-4bc4-8146-ca9974ffd855-0', tool_calls=[{'name': 'StudentAid_Federal_Search', 'args': {'__arg1': 'Aidvantage borrower complaint'}, 'id': 'call_nERYrnrUONQ7mP4q02u5anD6', 'type': 'tool_call'}], usage_metadata={'input_tokens': 314, 'output_tokens': 23, 'total_tokens': 337, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'action'\n",
      "Tool Used: StudentAid_Federal_Search\n",
      "=== PARSING RESULTS ===\n",
      "Tool calls: 0\n",
      "Tool results: 1\n",
      "Tools used: ['StudentAid_Federal_Search']\n",
      "\n",
      "=== TOOL CALLS ===\n",
      "\n",
      "=== TOOL RESULTS ===\n",
      "1. StudentAid_Federal_Search -> StudentAid.gov Search Results for: Aidvantage borrower complaint  Summary: Aidvantage is a student l...\n",
      "[ToolMessage(content=\"StudentAid.gov Search Results for: Aidvantage borrower complaint\\n\\nSummary: Aidvantage is a student loan servicer for federal loans; complaints can be submitted through the Help Center on their website. If you have issues, contact the U.S. Department of Educationâ€™s Federal Student Aid.\\n\\nOfficial Federal Information:\\n1. Aidvantage\\n   Aidvantage is an official student loan servicer of Federal Student Aid, assisting borrowers with their pre-defaulted loans. While your loan is serviced by Aidvantage, it will not be subject to forced ...\\n   URL: https://aidvantage.studentaid.gov/\\n\\n2. Contact us - Aidvantage - Federal Student Aid\\n   Federal Student Aid (FSA) is your federal loan provider. FSA uses servicers (private companies) like Aidvantage to manage billing, questions, and payments, and to help you enroll in the best repayment...\\n   URL: https://aidvantage.studentaid.gov/contact-us\\n\\n3. So Your Loan Was Transferredâ€”What's Next?\\n   | Loan Servicer | Website |\\n| --- | --- |\\n| Edfinancial | edfinancial.StudentAid.gov |\\n| MOHELA | mohela.StudentAid.gov |\\n| Aidvantage | aidvantage.StudentAid.gov |\\n| Nelnet | nelnet.StudentAid.gov |\\n...\\n   URL: https://studentaid.gov/articles/your-loan-was-transferred-whats-next/\\n\\n\", name='StudentAid_Federal_Search', id='d10e13af-e067-4508-8554-1cdbb697208b', tool_call_id='call_nERYrnrUONQ7mP4q02u5anD6')]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='Aidvantage is an official student loan servicer for federal loans, responsible for managing billing, questions, and payments for borrowers. According to the information available, complaints about Aidvantage can be submitted through their Help Center on their website. If borrowers experience issues with Aidvantage, they are advised to contact the U.S. Department of Educationâ€™s Federal Student Aid for assistance.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 77, 'prompt_tokens': 641, 'total_tokens': 718, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-BzawNyh3GG18yu9eKZ5hzIBm6OVyi', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--7ae26c49-6bdf-4fdc-934a-06f998553632-0', usage_metadata={'input_tokens': 641, 'output_tokens': 77, 'total_tokens': 718, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\" : [HumanMessage(content=\"What is the issue with Aidvantage in the borrower's complaint?\")]}\n",
    "\n",
    "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: '{node}'\")\n",
    "        if node == \"action\":\n",
    "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
    "          print_formatted_results(parse_logs(str(values[\"messages\"])))\n",
    "        print(values[\"messages\"])\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving update from node: 'agent'\n",
      "\n",
      "\n",
      "[AIMessage(content=\"To provide an accurate answer, I need to know the specific concerns the borrower has expressed about Nelnet's communication regarding their student loan issuer. Could you please provide more details or specify the concerns?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 317, 'total_tokens': 357, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-BzawPsaKdf4F5V0eCN0BX1FN1EOYD', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--02e4ccaf-df57-4667-878d-1451b9f18202-0', usage_metadata={'input_tokens': 317, 'output_tokens': 40, 'total_tokens': 357, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\" : [HumanMessage(content=\"What concerns does the borrower have regarding Nelnet's communication about their student loan issuer?\")]}\n",
    "\n",
    "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: '{node}'\")\n",
    "        if node == \"action\":\n",
    "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
    "          print_formatted_results(parse_logs(str(values[\"messages\"])))\n",
    "        print(\"\\n\")\n",
    "        print(values[\"messages\"])\n",
    "\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving update from node: 'agent'\n",
      "\n",
      "\n",
      "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_jZbSbFOH0gGCw5Laa8pMYnFt', 'function': {'arguments': '{\"__arg1\": \"NelNet auto-debit payment issues\"}', 'name': 'StudentAid_Federal_Search'}, 'type': 'function'}, {'id': 'call_02JtAJ0KsUzgPrBbtd9lI5Cj', 'function': {'arguments': '{\"__arg1\": \"borrower rights auto-debit payments\"}', 'name': 'StudentAid_Federal_Search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 328, 'total_tokens': 394, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-Bzb4uraB30ZLJZCLCVLoRrJd8X5T4', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--f1387126-d263-4436-b299-0d35967495f5-0', tool_calls=[{'name': 'StudentAid_Federal_Search', 'args': {'__arg1': 'NelNet auto-debit payment issues'}, 'id': 'call_jZbSbFOH0gGCw5Laa8pMYnFt', 'type': 'tool_call'}, {'name': 'StudentAid_Federal_Search', 'args': {'__arg1': 'borrower rights auto-debit payments'}, 'id': 'call_02JtAJ0KsUzgPrBbtd9lI5Cj', 'type': 'tool_call'}], usage_metadata={'input_tokens': 328, 'output_tokens': 66, 'total_tokens': 394, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'action'\n",
      "Tool Used: StudentAid_Federal_Search\n",
      "=== PARSING RESULTS ===\n",
      "Tool calls: 0\n",
      "Tool results: 2\n",
      "Tools used: ['StudentAid_Federal_Search']\n",
      "\n",
      "=== TOOL CALLS ===\n",
      "\n",
      "=== TOOL RESULTS ===\n",
      "1. StudentAid_Federal_Search -> StudentAid.gov Search Results for: NelNet auto-debit payment issues  Summary: Auto-debit for Nelnet ...\n",
      "2. StudentAid_Federal_Search -> StudentAid.gov Search Results for: borrower rights auto-debit payments  Summary: Auto-debit payments...\n",
      "\n",
      "=== CONTEXT EXTRACTION DEMO ===\n",
      "Extracted 10 contexts for evaluation:\n",
      "1. StudentAid.gov Search Results for: NelNet auto-debit payment issues...\n",
      "2. Summary: Auto-debit for Nelnet federal student loans does not occur during deferment or forbearance. If loans are paid ahead, auto debit will not happ...\n",
      "3. Official Federal Information:\n",
      "1. FAQ - Auto Debit - Nelnet - Federal Student Aid\n",
      "   For income-driven repayment (IDR) plans (Income-Based, Income-Cont...\n",
      "\n",
      "\n",
      "[ToolMessage(content='StudentAid.gov Search Results for: NelNet auto-debit payment issues\\n\\nSummary: Auto-debit for Nelnet federal student loans does not occur during deferment or forbearance. If loans are paid ahead, auto debit will not happen. Auto debit resumes after deferment or forbearance ends.\\n\\nOfficial Federal Information:\\n1. FAQ - Auto Debit - Nelnet - Federal Student Aid\\n   For income-driven repayment (IDR) plans (Income-Based, Income-Contingent, Pay As You Earn, Saving on a Valuable Education (formerly Revised Pay As You Earn)), or Reduced Payment Forbearance: Auto debi...\\n   URL: https://nelnet.studentaid.gov/content/faq/faqautodebit\\n\\n2. Credit Reporting - Nelnet - Federal Student Aid\\n   The U.S. Department of Education is contacting borrowers whose federal student loans are in good standing. Making on-time payments is essential to maintaining your credit.\\n\\nYou can stay on track by au...\\n   URL: https://nelnet.studentaid.gov/content/creditreporting\\n\\n3. Postpone Your Payments with Deferment or Forbearance\\n   If you are on the SAVE Plan or have a pending SAVE application, you may receive an email from the office of Federal Student Aid that your loans remain in forbearance, but interest will start accruing ...\\n   URL: https://nelnet.studentaid.gov/content/postponeyourpayments\\n\\n', name='StudentAid_Federal_Search', id='2139f554-d345-487f-ad8f-3465b99d2a69', tool_call_id='call_jZbSbFOH0gGCw5Laa8pMYnFt'), ToolMessage(content=\"StudentAid.gov Search Results for: borrower rights auto-debit payments\\n\\nSummary: Auto-debit payments help ensure timely student loan repayments, and often include a 0.25% interest rate reduction. Auto-debit is not available during deferment or forbearance periods. Set up auto-debit online to simplify payments.\\n\\nOfficial Federal Information:\\n1. Nelnet FAQ - Auto Debit - Nelnet - Federal Student Aid\\n   Auto debit is a convenient, simple payment option offering you the peace of mind that comes with knowing your student loan payments are being made accurately and on time.\\n\\nWe donâ€™t charge a service fe...\\n   URL: https://nelnet.studentaid.gov/content/faq/faqautodebit\\n\\n2. FAQ - Auto Debit - CRI - Federal Student Aid\\n   Generally, you are eligible to sign up for auto debit as long as your loan(s) is in repayment or will enter repayment within the next 180 days. To determine...\\n   URL: https://cri.studentaid.gov/content/faq/faqautodebit\\n\\n3. FAQs - Mohela - Federal Student Aid\\n   The U.S. Department of Education is contacting borrowers with delinquent federal student loans. Weâ€™re here to support you if you're having trouble making payments. Visit StudentAid.gov/loan-simulator ...\\n   URL: https://mohela.studentaid.gov/DL/FAQs/FAQ.aspx\\n\\n\", name='StudentAid_Federal_Search', id='e9c57c33-abf0-4ace-8456-a67111c41a5c', tool_call_id='call_02JtAJ0KsUzgPrBbtd9lI5Cj')]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'agent'\n",
      "\n",
      "\n",
      "[AIMessage(content=\"NelNet's repeated failure to process auto-debit payments, despite confirmation, can be concerning from a borrower rights perspective. Auto-debit payments are designed to ensure timely and accurate loan repayment, and borrowers have the right to expect that their payment instructions will be honored unless specific conditions, such as deferment or forbearance, are in place. When auto-debit fails without valid reason, it can lead to missed payments, potential credit damage, and increased borrower stress.\\n\\nFrom a legal standpoint, borrowers have the right to clear and accurate information about their repayment options and the expectation that their payments will be processed as agreed. Failures to process payments could potentially violate these rights, especially if they result in negative credit reporting or other adverse effects.\\n\\nRegarding privacy laws, auto-debit arrangements involve sensitive financial information. If NelNet's failure to process payments is related to mishandling or unauthorized access to borrower data, it could raise concerns about compliance with privacy laws such as the Gramm-Leach-Bliley Act or other data protection regulations. However, based on the available information, the issue appears to be more related to processing errors rather than privacy violations.\\n\\nIn summary, NelNet's failure to process auto-debit payments despite confirmation may infringe on borrower rights related to fair treatment and accurate information. If these failures are due to systemic issues or mishandling of data, they could also raise privacy concerns. Borrowers experiencing such issues should document their communications and consider reaching out to borrower rights advocates or legal counsel if necessary.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 304, 'prompt_tokens': 1022, 'total_tokens': 1326, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-Bzb524hvImTMtLPOtT8D36s0pVoUN', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--b0f23467-872e-40fb-9325-b08e5b7c3ce9-0', usage_metadata={'input_tokens': 1022, 'output_tokens': 304, 'total_tokens': 1326, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\" : [HumanMessage(content=\"How does NelNet's repeated failure to process auto-debit payments, despite confirmation, relate to borrower rights and potential violations of privacy laws?\")]}\n",
    "\n",
    "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: '{node}'\")\n",
    "        if node == \"action\":\n",
    "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
    "          # print_formatted_results(parse_logs(str(values[\"messages\"])))\n",
    "          parsed_data = parse_langchain_messages(values[\"messages\"])\n",
    "          print_formatted_results(parsed_data)\n",
    "        print(\"\\n\")\n",
    "        print(values[\"messages\"])\n",
    "\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\" : [HumanMessage(content=\"How does NelNet's repeated failure to process auto-debit payments, despite confirmation, relate to borrower rights and potential violations of privacy laws?\")]}\n",
    "response = simple_agent_graph.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Extracted 10 contexts for evaluation\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tool_calls_parser_for_eval\n",
    "importlib.reload(tool_calls_parser_for_eval)\n",
    "from tool_calls_parser_for_eval import parse_tool_call, parse_langchain_messages, extract_contexts_for_eval, print_formatted_results\n",
    "# from tool_calls_parser import parse_langchain_messages, print_formatted_results, extract_contexts_for_eval\n",
    "\n",
    "evaluation_contexts = extract_contexts_for_eval(response[\"messages\"])\n",
    "print(f\"âœ… Extracted {len(evaluation_contexts)} contexts for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PARSING RESULTS ===\n",
      "Tool calls: 2\n",
      "Tool results: 4\n",
      "Tools used: [None, 'Mohela_Servicer_Search', 'StudentAid_Federal_Search']\n",
      "\n",
      "=== TOOL CALLS ===\n",
      "1. StudentAid_Federal_Search -> {'__arg1': 'NelNet auto-debit payment issues borrower rights privacy laws'}\n",
      "2. Mohela_Servicer_Search -> {'__arg1': 'NelNet auto-debit payment issues borrower rights privacy laws'}\n",
      "\n",
      "=== TOOL RESULTS ===\n",
      "1. None -> How does NelNet's repeated failure to process auto-debit payments, despite confirmation, relate to b...\n",
      "2. StudentAid_Federal_Search -> StudentAid.gov Search Results for: NelNet auto-debit payment issues borrower rights privacy laws  Su...\n",
      "3. Mohela_Servicer_Search -> Mohela Search Results for: NelNet auto-debit payment issues borrower rights privacy laws  Summary: M...\n",
      "4. None -> NelNet handles auto-debit payments for federal student loans and is committed to protecting borrower...\n",
      "\n",
      "=== CONTEXT EXTRACTION DEMO ===\n",
      "Extracted 10 contexts for evaluation:\n",
      "1. StudentAid.gov Search Results for: NelNet auto-debit payment issues borrower rights privacy laws...\n",
      "2. Summary: Nelnet handles auto-debit for federal student loans; borrower rights are protected by privacy laws; avoid scams by not sharing personal info....\n",
      "3. Official Federal Information:\n",
      "1. FAQ - Auto Debit - Nelnet - Federal Student Aid\n",
      "   Federal Student Aid (FSA) is actively assessing the impact of the ...\n"
     ]
    }
   ],
   "source": [
    "parsed_data = parse_langchain_messages(response[\"messages\"])\n",
    "print_formatted_results(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ EVALUATION SAMPLE:\n",
      "Query: How does NelNet's repeated failure to process auto-debit payments, despite confirmation, relate to borrower rights and potential violations of privacy laws?\n",
      "Response: NelNet handles auto-debit payments for federal student loans and is committed to protecting borrower rights and privacy laws. According to official federal information, NelNet provides FAQs and resour...\n",
      "Contexts: 10 extracted\n",
      "Tools: [None, 'Mohela_Servicer_Search', 'StudentAid_Federal_Search']\n"
     ]
    }
   ],
   "source": [
    "eval_sample = {\n",
    "    \"user_input\": inputs[\"messages\"][0].content,\n",
    "    \"response\": response[\"messages\"][-1].content,  # Final AI response\n",
    "    \"retrieved_contexts\": evaluation_contexts,\n",
    "    \"tools_used\": parsed_data['summary']['tools'],\n",
    "    \"num_contexts\": len(evaluation_contexts)\n",
    "}\n",
    "\n",
    "print(f\"\\nðŸŽ¯ EVALUATION SAMPLE:\")\n",
    "print(f\"Query: {eval_sample['user_input']}\")\n",
    "print(f\"Response: {eval_sample['response'][:200]}...\")\n",
    "print(f\"Contexts: {eval_sample['num_contexts']} extracted\")\n",
    "print(f\"Tools: {eval_sample['tools_used']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
